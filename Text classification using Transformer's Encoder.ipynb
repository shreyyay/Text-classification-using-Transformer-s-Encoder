{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc4273feb0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Word Embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.embed(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Positional Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, max_seq_len = 60):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, model_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, model_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/model_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/model_dim)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = x * math.sqrt(self.model_dim)\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = x + self.pe[:,:seq_len].clone().detach()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, model_dim,dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.d_k = model_dim//heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(model_dim, model_dim,bias=False)\n",
    "        self.k_linear = nn.Linear(model_dim, model_dim,bias=False)\n",
    "        self.v_linear = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        self.out  = nn.Linear(model_dim,model_dim,bias=False)\n",
    "        \n",
    "    def forward(self, q,k,v):\n",
    "        \n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        \n",
    "       \n",
    "        k  = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        scores = attention(q, k, v, self.d_k, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.model_dim)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feed forward sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, d_ff=512, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.linear_1 = nn.Linear(model_dim, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, model_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, model_dim, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = model_dim\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) /(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(model_dim)\n",
    "        self.norm_2 = Norm(model_dim)\n",
    "        self.attn = MultiHeadAttention(heads, model_dim)\n",
    "        self.ff = FeedForward(model_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Attention Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim,heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = Embedder(vocab_size, model_dim)\n",
    "        self.pe = PositionalEncoder(model_dim)\n",
    "        self.encode = EncoderLayer(model_dim, heads)\n",
    "        self.norm = Norm(model_dim)\n",
    "    def forward(self, src):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        x = self.encode(x)\n",
    "        return self.norm(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config,src_vocab):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        h = self.config.h\n",
    "        model_dim = self.config.model_dim\n",
    "        self.encoder = Encoder(src_vocab, model_dim,h)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.out = nn.Linear(self.config.model_dim, self.config.output_size)\n",
    "    def forward(self, src):\n",
    "        e_outputs = self.encoder(src)\n",
    "        e_outputs = e_outputs[:,-1,:]\n",
    "        output = self.out(e_outputs)\n",
    "        return self.softmax(output )\n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "                \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Reduce learning rate as number of epochs increase\n",
    "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
    "            self.reduce_lr()\n",
    "            \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            x = batch.text\n",
    "            y = (batch.label - 1).type(torch.LongTensor)\n",
    "            y_pred = self.__call__(x)\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "                losses = []\n",
    "                \n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy = evaluate_model(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                self.train()\n",
    "                \n",
    "        return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \n",
    "    model_dim = 256\n",
    "    h = 8\n",
    "    output_size = 4\n",
    "    lr = 0.0003\n",
    "    max_epochs = 35\n",
    "    batch_size = 128\n",
    "    max_sen_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_pandas_df(self, filename):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        \n",
    "        '''\n",
    "        with open(filename, 'r') as file:\n",
    "            \n",
    "            data = [line.strip().split(',', maxsplit=1) for line in file]\n",
    "            data_text = list(map(lambda x: x[1], data))\n",
    "            data_label = list(map(lambda x: x[0].strip()[-1], data))\n",
    "\n",
    "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "        return full_df\n",
    "    \n",
    "    def load_data(self, train_file, test_file=None, val_file=None):\n",
    "        \n",
    "\n",
    "        NLP = spacy.load('en_core_web_sm')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, batch_first= True,lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file)\n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        test_df = self.get_pandas_df(test_file)\n",
    "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, datafields)\n",
    "        \n",
    "        \n",
    "        train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data)\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
    "        print (\"Loaded {} validation examples\".format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "\n",
    "        x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7788 training examples\n",
      "Loaded 783 test examples\n",
      "Loaded 1947 validation examples\n",
      "Epoch: 0\n",
      "\tAverage training loss: -0.25666\n",
      "\tVal Accuracy: 0.2722\n",
      "Epoch: 1\n",
      "\tAverage training loss: -0.27707\n",
      "\tVal Accuracy: 0.3071\n",
      "Epoch: 2\n",
      "\tAverage training loss: -0.44610\n",
      "\tVal Accuracy: 0.4212\n",
      "Epoch: 3\n",
      "\tAverage training loss: -0.49375\n",
      "\tVal Accuracy: 0.4710\n",
      "Epoch: 4\n",
      "\tAverage training loss: -0.59122\n",
      "\tVal Accuracy: 0.5265\n",
      "Epoch: 5\n",
      "\tAverage training loss: -0.57599\n",
      "\tVal Accuracy: 0.5501\n",
      "Epoch: 6\n",
      "\tAverage training loss: -0.64595\n",
      "\tVal Accuracy: 0.5670\n",
      "Epoch: 7\n",
      "\tAverage training loss: -0.64669\n",
      "\tVal Accuracy: 0.5763\n",
      "Epoch: 8\n",
      "\tAverage training loss: -0.63635\n",
      "\tVal Accuracy: 0.6091\n",
      "Epoch: 9\n",
      "\tAverage training loss: -0.81618\n",
      "\tVal Accuracy: 0.6364\n",
      "Epoch: 10\n",
      "\tAverage training loss: -0.79254\n",
      "\tVal Accuracy: 0.6662\n",
      "Epoch: 11\n",
      "Reducing LR\n",
      "\tAverage training loss: -0.81670\n",
      "\tVal Accuracy: 0.6610\n",
      "Epoch: 12\n",
      "\tAverage training loss: -0.81410\n",
      "\tVal Accuracy: 0.6687\n",
      "Epoch: 13\n",
      "\tAverage training loss: -0.83793\n",
      "\tVal Accuracy: 0.6826\n",
      "Epoch: 14\n",
      "\tAverage training loss: -0.84015\n",
      "\tVal Accuracy: 0.6708\n",
      "Epoch: 15\n",
      "\tAverage training loss: -0.87707\n",
      "\tVal Accuracy: 0.6749\n",
      "Epoch: 16\n",
      "\tAverage training loss: -0.87715\n",
      "\tVal Accuracy: 0.6805\n",
      "Epoch: 17\n",
      "\tAverage training loss: -0.87008\n",
      "\tVal Accuracy: 0.6805\n",
      "Epoch: 18\n",
      "\tAverage training loss: -0.83704\n",
      "\tVal Accuracy: 0.6728\n",
      "Epoch: 19\n",
      "\tAverage training loss: -0.86918\n",
      "\tVal Accuracy: 0.6775\n",
      "Epoch: 20\n",
      "\tAverage training loss: -0.84032\n",
      "\tVal Accuracy: 0.6790\n",
      "Epoch: 21\n",
      "\tAverage training loss: -0.86109\n",
      "\tVal Accuracy: 0.6831\n",
      "Epoch: 22\n",
      "\tAverage training loss: -0.89965\n",
      "\tVal Accuracy: 0.6826\n",
      "Epoch: 23\n",
      "Reducing LR\n",
      "\tAverage training loss: -0.89437\n",
      "\tVal Accuracy: 0.6826\n",
      "Epoch: 24\n",
      "\tAverage training loss: -0.90409\n",
      "\tVal Accuracy: 0.6836\n",
      "Epoch: 25\n",
      "\tAverage training loss: -0.87778\n",
      "\tVal Accuracy: 0.6795\n",
      "Epoch: 26\n",
      "\tAverage training loss: -0.84807\n",
      "\tVal Accuracy: 0.6810\n",
      "Epoch: 27\n",
      "\tAverage training loss: -0.88896\n",
      "\tVal Accuracy: 0.6826\n",
      "Epoch: 28\n",
      "\tAverage training loss: -0.85091\n",
      "\tVal Accuracy: 0.6877\n",
      "Epoch: 29\n",
      "\tAverage training loss: -0.87269\n",
      "\tVal Accuracy: 0.6862\n",
      "Epoch: 30\n",
      "\tAverage training loss: -0.87413\n",
      "\tVal Accuracy: 0.6831\n",
      "Epoch: 31\n",
      "\tAverage training loss: -0.92616\n",
      "\tVal Accuracy: 0.6800\n",
      "Epoch: 32\n",
      "\tAverage training loss: -0.89756\n",
      "\tVal Accuracy: 0.6780\n",
      "Epoch: 33\n",
      "\tAverage training loss: -0.92002\n",
      "\tVal Accuracy: 0.6841\n",
      "Epoch: 34\n",
      "\tAverage training loss: -0.93559\n",
      "\tVal Accuracy: 0.6836\n",
      "Final Training Accuracy: 0.8950\n",
      "Final Validation Accuracy: 0.6903\n",
      "Final Test Accuracy: 0.6794\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    config = Config()\n",
    "    train_file = '/home/shrey/Documents/NLP reserach papers/tran'\n",
    "    \n",
    "    test_file = '/home/shrey/Documents/NLP reserach papers/test'\n",
    "    \n",
    "    \n",
    "    dataset = Dataset(config)\n",
    "    dataset.load_data('/home/shrey/Documents/NLP reserach papers/tran', '/home/shrey/Documents/NLP reserach papers/test')\n",
    "    \n",
    "    model = Transformer(config, len(dataset.vocab))\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    NLLLoss = nn.NLLLoss()\n",
    "    model.add_optimizer(optimizer)\n",
    "    model.add_loss_op(NLLLoss)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for i in range(config.max_epochs):\n",
    "        print (\"Epoch: {}\".format(i))\n",
    "        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "    val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "    test_acc = evaluate_model(model, dataset.test_iterator)\n",
    "\n",
    "    print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "    print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "    print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
